{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9abbfac-445a-4ae9-a729-c17d76007932",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "%pip install azure-eventhub\n",
    "%pip install nest-asyncio\n",
    "%pip install vaderSentiment\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from azure.eventhub.aio import EventHubConsumerClient\n",
    "from pyspark.sql.functions import col, udf, avg\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, TimestampType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Install nest_asyncio to allow the event loop to run in a Databricks notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Create Spark Session if not already created\n",
    "spark = SparkSession.builder.appName(\"EventHubSentimentAnalysis\").getOrCreate()\n",
    "\n",
    "# Setting Up the Event Hub Consumer\n",
    "CONNECTION_STR = \"Endpoint=sb://youtube-comments-namespace.servicebus.windows.net/;SharedAccessKeyName=ListenPolicy;SharedAccessKey=ACCESS_KEY;EntityPath=youtube-comment-stream\"\n",
    "EVENTHUB_NAME = \"youtube-comment-stream\"\n",
    "CONSUMER_GROUP = \"$Default\"\n",
    "\n",
    "# Define schema for incoming Event Hub messages\n",
    "schema = StructType([\n",
    "    StructField(\"channelId\", StringType(), True),\n",
    "    StructField(\"videoId\", StringType(), True),\n",
    "    StructField(\"textDisplay\", StringType(), True),\n",
    "    StructField(\"authorDisplayName\", StringType(), True),\n",
    "    StructField(\"likeCount\", IntegerType(), True),\n",
    "    StructField(\"publishedAt\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "# VADER Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Delta Lake path\n",
    "output_delta_path = \"/mnt/delta/youtube_comments\"\n",
    "\n",
    "# Define the message limit if you want to stop after a specific number\n",
    "message_limit = 1000\n",
    "message_count = 0\n",
    "\n",
    "# UDF for sentiment analysis\n",
    "def get_sentiment(text):\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment['compound']  # Only return the compound score\n",
    "\n",
    "get_sentiment_udf = udf(get_sentiment, StringType())\n",
    "\n",
    "# Async function to fetch and process each message\n",
    "async def on_event(partition_context, event):\n",
    "    global message_count\n",
    "    message_count += 1\n",
    "\n",
    "    event_data = event.body_as_str()  # Extract event data\n",
    "    print(f\"Received message {message_count} from partition {partition_context.partition_id}: {event_data}\")\n",
    "    \n",
    "    # Parse the received message into a DataFrame for processing\n",
    "    received_df = spark.read.json(spark.sparkContext.parallelize([event_data]), schema=schema)\n",
    "\n",
    "    # Data cleaning: Remove null values and duplicates\n",
    "    cleaned_comments_df = received_df.filter(col(\"textDisplay\").isNotNull()).dropDuplicates()\n",
    "\n",
    "    # Apply Sentiment Analysis on comments\n",
    "    sentiment_comments_df = cleaned_comments_df.withColumn(\"sentiment\", get_sentiment_udf(col(\"textDisplay\")))\n",
    "\n",
    "    # Write the DataFrame to Delta table\n",
    "    sentiment_comments_df.write.format(\"delta\").mode(\"append\").save(output_delta_path)\n",
    "    print(f\"Data written to Delta Lake for message {message_count}\")\n",
    "\n",
    "    # Update checkpoint after processing each message\n",
    "    await partition_context.update_checkpoint(event)\n",
    "    \n",
    "    # Check if message limit is reached, stop receiving more events\n",
    "    if message_count >= message_limit:\n",
    "        print(f\"Message limit {message_limit} reached. Stopping event processing.\")\n",
    "        raise asyncio.CancelledError()\n",
    "\n",
    "# Function to handle events\n",
    "async def receive_events():\n",
    "    client = EventHubConsumerClient.from_connection_string(\n",
    "        conn_str=CONNECTION_STR,\n",
    "        consumer_group=CONSUMER_GROUP,\n",
    "        eventhub_name=EVENTHUB_NAME\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Receive events continuously and process them in real-time\n",
    "        await client.receive(\n",
    "            on_event=on_event,\n",
    "            starting_position=\"-1\",  # Start from the earliest available event\n",
    "            starting_position_inclusive=True\n",
    "        )\n",
    "    except asyncio.CancelledError:\n",
    "        print(\"Event receiving stopped due to message limit.\")\n",
    "    finally:\n",
    "        print(\"Closing Event Hub consumer.\")\n",
    "        await client.close()\n",
    "\n",
    "# Main function\n",
    "async def main():\n",
    "    await receive_events()\n",
    "\n",
    "# Start the process\n",
    "asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "adb-youtube-sentiment-analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
